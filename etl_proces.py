# -*- coding: utf-8 -*-
"""ETL proces.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l2a3uEjckpl3scWqGlMWx7O1pnft9vlF
"""

import json
from google.colab import userdata
userdata.get('GOOGLE_CREDENTIALS_JSON')

credentials_json_string = userdata.get('GOOGLE_CREDENTIALS_JSON')
credentials_dict = json.loads(credentials_json_string)

import pandas as pd
import os
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# ========== Setup ==========

# Buat folder untuk data hasil pembersihan
os.makedirs("data_clean", exist_ok=True)

# Scope dan kredensial
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
try:
    credentials = ServiceAccountCredentials.from_json_keyfile_dict(credentials_dict, scope)
    client = gspread.authorize(credentials)
    print("✅ Autentikasi Google Sheets berhasil.")
except Exception as e:
    print(f"❌ Gagal melakukan autentikasi Google Sheets: {e}")
    exit()

# ID Google Spreadsheet
SPREADSHEET_ID_ASET = "1bbXV377Gu4MxJzbRWxFr_kbSZyzyP80kCyhR80TDWzQ"
SHEET_NAME_ASET = "Datek Aset All"
SPREADSHEET_ID_USER = "1LMyZprJ_w3X6DC7Jqu0CpJaVQ3u8VUEMuSEADv1EMjc"
SHEET_NAME_USER = "Data All"


# ========== Extract ==========

def load_sheet_as_dataframe(spreadsheet_id, sheet_name):
    """Memuat data dari Google Sheet ke Pandas DataFrame."""
    try:
        print(f"ℹ️ Mencoba memuat sheet '{sheet_name}' dari Spreadsheet ID: {spreadsheet_id}")
        sheet = client.open_by_key(spreadsheet_id).worksheet(sheet_name)
        data = sheet.get_all_values()
        if len(data) > 1:
            df = pd.DataFrame(data[1:], columns=data[0])
            print(f"✅ Berhasil memuat {len(df)} baris data dari sheet '{sheet_name}'.")
            return df
        elif len(data) == 1:
             print(f"⚠️ Sheet '{sheet_name}' hanya berisi header.")
             return pd.DataFrame(columns=data[0])
        else:
            print(f"⚠️ Sheet '{sheet_name}' kosong.")
            return pd.DataFrame()
    except gspread.exceptions.WorksheetNotFound:
        print(f"❌ Gagal memuat: Worksheet '{sheet_name}' tidak ditemukan di Spreadsheet ID {spreadsheet_id}.")
        return pd.DataFrame()
    except Exception as e:
        print(f"❌ Gagal memuat sheet '{sheet_name}': {e}")
        return pd.DataFrame()

raw_aset_data = load_sheet_as_dataframe(SPREADSHEET_ID_ASET, SHEET_NAME_ASET)
aset_data = load_sheet_as_dataframe(SPREADSHEET_ID_ASET, SHEET_NAME_ASET)
user_data = load_sheet_as_dataframe(SPREADSHEET_ID_USER, SHEET_NAME_USER)

# ========== Transform (Rename Duplicate Columns) ==========

if not aset_data.empty:
    print("\n📋 Info aset_data SEBELUM rename kolom:")
    aset_data.info()
    print("\nKolom asli:", list(aset_data.columns))

    target_col_name = "Status OSP AMARTA"
    new_cols = []
    count = 1
    duplicates_found = False

    for col in aset_data.columns:
        current_col_name = col
        if col == target_col_name:
            new_name = f"{target_col_name} {count}"
            new_cols.append(new_name)
            if count > 1:
                 duplicates_found = True
            count += 1
        else:
            new_cols.append(col)

    # Hanya ganti nama kolom jika memang ditemukan duplikat (count > 2 berarti ada minimal 2 kolom target)
    if duplicates_found:
        aset_data.columns = new_cols
        print(f"\n✅ Kolom duplikat '{target_col_name}' berhasil di-rename.")
    elif count == 2:
         print(f"\nℹ️ Hanya ditemukan satu kolom '{target_col_name}'. Tidak ada rename duplikat yang dilakukan.")
    else:
        print(f"\n⚠️ Kolom '{target_col_name}' tidak ditemukan dalam data aset.")


    # Menampilkan info setelah potensi perubahan kolom
    print("\n📋 Info aset_data SETELAH rename kolom:")
    aset_data.info()
    print("\nKolom setelah rename:", list(aset_data.columns))

else:
    print("\n⚠️ DataFrame aset_data kosong, proses rename dilewati.")
user_data.info()

import re
def clean_column_names(df):
    df.columns = [re.sub(r"\s+", " ", col).strip() for col in df.columns]
    return df

aset_data = clean_column_names(aset_data)
user_data = clean_column_names(user_data)


# # ========== Transform ==========

def capitalize_columns_except(df, exclude_columns):
    """
    Capitalize each word in string columns, except for the columns specified in exclude_columns.
    Cleans column names to remove excessive whitespace as well.
    """
    if df.empty:
        return df

    # Bersihkan nama kolom (strip dan ubah spasi ganda jadi 1 spasi)
    df.columns = [re.sub(r"\s+", " ", col).strip() for col in df.columns]

    for col in df.columns:
        if col not in exclude_columns:
            try:
                df[col] = df[col].apply(lambda x: x.title() if isinstance(x, str) else x)
            except Exception as e:
                print(f"⚠️ Gagal memproses kolom '{col}': {e}")
    return df

# Kolom yang tidak ingin diubah
exclude_columns = ["Hostname OLT", "FDT ID", "FATID", "Type OLT", "OLT", "ID FAT", "CLEANSING HP", "FAT ID X", "LINK DOKUMEN FEEDER", "LINK DATA ASET", "LINK MAPS"]

# # Bersihkan aset_data
if not aset_data.empty:
    aset_data.drop_duplicates(subset=["FATID"], keep="first", inplace=True)
    aset_data = capitalize_columns_except(aset_data, exclude_columns)
aset_data.head()
aset_data.info()

def clean_dataframe(df, exclude_columns):
    """
    Bersihkan header kolom dan kapitalisasi isi string, kecuali kolom di exclude_columns.
    """
    if df.empty:
        return df

    # Bersihkan nama kolom: hilangkan spasi ekstra dan strip
    df.columns = [re.sub(r"\s+", " ", col).strip() for col in df.columns]

    # Bersihkan isi string di setiap kolom, lalu kapitalisasi jika bukan di exclude_columns
    for col in df.columns:
        if col not in exclude_columns:
            try:
                df[col] = df[col].apply(lambda x: re.sub(r"\s+", " ", x.strip()).title() if isinstance(x, str) else x)
            except Exception as e:
                print(f"⚠️ Gagal memproses kolom '{col}': {e}")
    return df

# # Bersihkan user_data
if not user_data.empty:
    user_data = (
        user_data.sort_values(
            by=["SID", "Cust Name", "Koodinat Pelanggan"],
            ascending=False,
            na_position="last"
        )
        .drop_duplicates(subset=["ID Permohonan"], keep="first")
    )
    user_data = clean_dataframe(user_data, exclude_columns)
    kolom_yang_dihapus = [
    'Koordinat FAT', 'Hostname OLT', 'FDT'
]
    user_data.drop(columns=kolom_yang_dihapus, inplace=True, errors='ignore')

# ========== Load (Opsional Simpan ke File) ==========

# aset_data.to_csv("data_clean/aset_data_clean.csv", index=False)
# user_data.to_csv("data_clean/user_data_clean.csv", index=False)

# ========== Log ==========

print("📋 Info aset_data setelah transformasi:")
print(aset_data.info())
print("\n📋 Info user_data setelah transformasi:")
print(user_data.info())

aset_data.rename(columns={
    # Kolom dari Tabel OLT
    "Hostname OLT": "hostname_olt",
    "Kordinat OLT": "koordinat_olt",
    "Brand OLT": "brand_olt",
    "Type OLT": "type_olt",
    "Kapasitas OLT": "kapasitas_olt",
    "Kapasitas port OLT": "kapasitas_port_olt",
    "OLT Port": "olt_port",
    "OLT": "olt",
    "Interface OLT": "interface_olt",
    "Lokasi OLT": "lokasi_olt",

    # Kolom dari Tabel FDT
    "FDT ID": "fdt_id",
    "Status OSP AMARTA 1": "status_osp_amarta_fdt",
    "Jumlah Splitter FDT": "jumlah_splitter_fdt",
    "Kapasitas Splitter FDT": "kapasitas_splitter_fdt",
    "FDT New/Existing": "fdt_new_existing",
    "Port FDT": "port_fdt",
    "Koodinat FDT": "koordinat_fdt",

    # Kolom dari Tabel FAT
    "FATID": "fat_id",
    "Jumlah Splitter FAT": "jumlah_splitter_fat",
    "Kapasitas Splitter FAT": "kapasitas_splitter_fat",
    "Koodinat FAT": "koordinat_fat",
    "Status OSP AMARTA FAT": "status_osp_amarta_fat",
    "FAT KONDISI": "fat_kondisi",
    "FAT FILTER PEMAKAIAN": "fat_filter_pemakaian",
    "KETERANGAN FULL": "keterangan_full",
    "FAT ID X": "fat_id_x",
    "FILTER FAT CAP": "filter_fat_cap",
    "Status OSP AMARTA 2": "status_osp_amarta_fat",

    # Kolom dari Tabel Cluster
    "Cluster": "cluster",
    "Koordinat Cluster": "koordinat_cluster",
    "Area KP": "area_kp",
    "Kota/Kab": "kota_kab",
    "Kecamatan": "kecamatan",
    "Kelurahan": "kelurahan",
    "UP3": "up3",
    "ULP": "ulp",

    # Kolom dari Tabel Dokumentasi
    "LINK DOKUMEN FEEDER": "link_dokumen_feeder",
    "KETERANGAN DOKUMEN": "keterangan_dokumen",
    "LINK DATA ASET": "link_data_aset",
    "KETERANGAN DATA ASET": "keterangan_data_aset",
    "LINK MAPS": "link_maps",
    "UPDATE ASET": "update_aset",
    "AMARTA UPDATE": "amarta_update",

    # Kolom dari Tabel HomeConnected
    "HC OLD": "hc_old",
    "HC iCRM+": "hc_icrm",
    "TOTAL HC": "total_hc",
    "CLEANSING HP": "cleansing_hp",

    # Kolom dari Tabel AdditionalInformation
    "PA": "pa",
    "Tanggal RFS": "tanggal_rfs",
    "Mitra": "mitra",
    "Kategori": "kategori",
    "Sumber Datek": "sumber_datek"
}, inplace=True)

user_data.rename(columns={
    "SID": "sid",
    "ID Permohonan": "id_permohonan",
    "Koodinat Pelanggan": "koordinat_pelanggan",
    "Cust Name": "cust_name",
    "Telpn": "telpn",
    "ID FAT": "fat_id",
    "NOTES": "notes"
}, inplace=True)
aset_data.info()
user_data.info()

print("\n🔄 Memulai konversi tipe data...")

# --- Handle aset_data ---
if 'aset_data' in locals() and isinstance(aset_data, pd.DataFrame) and not aset_data.empty:
    try:
        print("\n--- Memproses aset_data ---")

        # 1. Handle 'tanggal_rfs': Koreksi Spesifik '0203'
        if 'tanggal_rfs' in aset_data.columns:
            print("  -> Memproses kolom 'tanggal_rfs': Mengganti '0203' dengan '2023'.")
            # Pastikan kolom diperlakukan sebagai string untuk operasi replace
            aset_data['tanggal_rfs'] = aset_data['tanggal_rfs'].astype(str)
            # Lakukan penggantian spesifik
            aset_data['tanggal_rfs'] = aset_data['tanggal_rfs'].str.replace('0203', '2023', regex=False)
            # Konversi ke datetime akan dilakukan di astype() di bagian bawah jika dimasukkan di dict
            print("    ✅ Pembersihan string '0203' di 'tanggal_rfs' selesai.")


        # 2. Define the original dictionary with all intended types
        # Kolom-kolom Int64 di sini akan diproses secara terpisah terlebih dahulu
        original_astype_dict_aset = {
             # Tabel OLT
            "hostname_olt": "string", "Koordinat_olt": "string", "brand_olt": "string", "type_olt": "string",
            "kapasitas_olt": "Int64", # Target Int64 - akan diproses khusus
            "kapasitas_port_olt": "Int64", # Target Int64 - akan diproses khusus
            "olt_port": "Int64", # Target Int64 - akan diproses khusus
            "olt": "string", "interface_olt": "string",
            # Tabel FDT
            "fdt_id": "string", "status_osp_amarta_fdt": "string", "jumlah_splitter_fdt": "Int64", # Target Int64 - akan diproses khusus
            "kapasitas_splitter_fdt": "Int64", # Target Int64 - akan diproses khusus
            "fdt_new_existing": "string", "port_fdt": "Int64", # Target Int64 - akan diproses khusus
            "koordinat_fdt": "string",
            # Tabel FAT
            "fat_id": "string", "jumlah_splitter_fat": "Int64", # Target Int64 - akan diproses khusus
            "kapasitas_splitter_fat": "Int64", # Target Int64 - akan diproses khusus
            "koordinat_fat": "string",
            "status_osp_amarta_fat": "string", "fat_kondisi": "string", "fat_filter_pemakaian": "string", "keterangan_full": "string",
            "fat_id_x": "string", "filter_fat_cap": "string",
            # Tabel Cluster
            "cluster":"string","koordinat_cluster": "string", "area_kp": "string", "kota_kab": "string", "kecamatan": "string", "kelurahan": "string",
            "up3": "string", "ulp": "string",
            # Dokumentasi
            "link_dokumen_feeder": "string", "keterangan_dokumen": "string", "link_data_aset": "string", "keterangan_data_aset": "string",
            "link_maps": "string", "update_aset": "string", "amarta_update": "string",
            # HomeConnected
            "hc_old": "Int64", # Target Int64 - akan diproses khusus
            "hc_icrm": "Int64", # Target Int64 - akan diproses khusus
            "total_hc": "Int64", # Target Int64 - akan diproses khusus
            "cleansing_hp": "string",
            # Additional Info
            "pa": "string",
            "tanggal_rfs":"datetime64[ns]", # Target datetime - akan diproses di astype umum
            "mitra": "string", "kategori": "string", "sumber_datek": "string"
        }

        # Identifikasi kolom-kolom yang ditargetkan untuk Int64
        int64_cols_to_process = [col for col, dtype in original_astype_dict_aset.items() if dtype == 'Int64']
        other_cols_to_astype = {col: dtype for col, dtype in original_astype_dict_aset.items() if dtype != 'Int64'}

        print("\n  --- Memproses Kolom Target Int64 ---")
        processed_int64_cols = [] # Untuk melacak kolom yang berhasil diproses
        for col in int64_cols_to_process:
            if col in aset_data.columns:
                print(f"  -> Memproses kolom '{col}': Membersihkan dan konversi ke Int64.")
                try:
                    # Hitung jumlah nilai yang tidak kosong sebelum diproses
                    initial_non_null_count = aset_data[col].notnull().sum()
                    # print(f"    -> Awalnya terdapat {initial_non_null_count} nilai non-kosong.") # Opsional detail

                    # Pastikan kolom string untuk operasi string
                    cleaned_series = aset_data[col].astype(str).copy()

                    # Hapus karakter titik, koma, atau spasi yang mungkin ada
                    # Regex [., ] akan menghapus titik, koma, atau spasi
                    # Tambahkan strip() untuk menghapus spasi di awal/akhir string
                    cleaned_series = cleaned_series.str.replace('[., ]', '', regex=True).str.strip()

                    # Konversi ke Int64 (ini juga menangani NaNs dari coerce)
                    numeric_series = pd.to_numeric(cleaned_series, errors='coerce')
                    aset_data[col] = numeric_series.astype('Int64')

                    # Hitung jumlah nilai yang tidak kosong setelah diproses
                    final_non_null_count = aset_data[col].notnull().sum()
                    # print(f"    -> Setelah pembersihan dan konversi, terdapat {final_non_null_count} nilai non-kosong.") # Opsional detail

                    # Hitung berapa banyak nilai yang awalnya tidak kosong menjadi kosong (NaN)
                    num_converted_to_nan = initial_non_null_count - final_non_null_count

                    if num_converted_to_nan > 0:
                        print(f"    ⚠️ Terdapat {num_converted_to_nan} nilai di '{col}' yang awalnya tidak kosong namun diubah menjadi kosong (NaN).")
                    else:
                         print(f"    ✅ Tidak ada nilai di '{col}' yang awalnya tidak kosong diubah menjadi kosong (NaN).")

                    print(f"    ✅ Pembersihan dan konversi '{col}' ke Int64 selesai.")
                    processed_int64_cols.append(col) # Tandai kolom ini berhasil diproses

                except Exception as col_e:
                     print(f"    ❌ Gagal memproses kolom '{col}' secara spesifik: {col_e}")
                     print(f"    ℹ️ Kolom '{col}' mungkin tidak dikonversi ke Int64 karena error.")
            else:
                 print(f"  -> Kolom '{col}' tidak ditemukan di aset_data. Dilewati.")


        print("\n  --- Mengonversi Kolom Lainnya ---")
        # Filter dictionary untuk hanya menyertakan kolom yang masih perlu di-astype
        # Yaitu kolom yang bukan Int64 target awalnya ATAU kolom Int64 target yang gagal diproses di atas
        valid_astype_dict_aset = {}
        missing_cols_aset = []
        for col, dtype in other_cols_to_astype.items():
             if col in aset_data.columns:
                 valid_astype_dict_aset[col] = dtype
             else:
                 missing_cols_aset.append(col)

        # Cek kembali kolom Int64 yang GAGAL diproses.
        # Secara default, jika gagal, tipenya tidak akan berubah dari object/string.
        # Kita bisa biarkan saja di sini atau tambahkan penanganan khusus jika perlu.
        # Untuk saat ini, biarkan saja, dan info() di akhir akan menunjukkan tipenya.

        if missing_cols_aset:
            print(f"    ⚠️ Kolom berikut tidak ditemukan di aset_data dan dilewati dalam konversi tipe data umum: {missing_cols_aset}")

        if valid_astype_dict_aset:
            print("  -> Mengonversi tipe data kolom aset_data lainnya menggunakan astype...")
            # Lakukan astype untuk kolom yang tersisa di dictionary (termasuk tanggal_rfs ke datetime)
            try:
                aset_data = aset_data.astype(valid_astype_dict_aset)
                print("  ✅ Konversi tipe data aset_data (selain Int64 yang sudah ditangani) selesai.")
            except Exception as astype_e:
                 # Menangkap error spesifik yang mungkin terjadi di langkah astype()
                 print(f"❌ Error saat konversi tipe data aset_data pada langkah astype() untuk kolom-kolom tersisa: {astype_e}")
                 print("    ℹ️ Pastikan format data di kolom yang tersisa sesuai dengan tipe data target di dictionary.")


        else:
            print("  ℹ️ Tidak ada kolom lain di aset_data yang perlu dikonversi via astype.")

    except Exception as e:
        # Menangkap error tak terduga di luar blok try/except kolom spesifik atau astype umum
        print(f"❌ Error tak terduga saat konversi tipe data aset_data secara keseluruhan: {e}")

else:
    print("⚠️ DataFrame aset_data kosong atau tidak terdefinisi, konversi tipe data dilewati.")


# --- Handle user_data --- (Kode konversi user_data tetap sama)
# ... (bagian kode user_data dari jawaban sebelumnya) ...
try:
    if 'user_data' in locals() and isinstance(user_data, pd.DataFrame) and not user_data.empty:
        print("\n--- Memproses user_data ---")
        print("  -> Mengonversi kolom user_data...")
        try:
            astype_dict_user = {
                "sid": "string", "id_permohonan": "string", "koordinat_pelanggan": "string",
                "cust_name": "string", "telpn": "string", "fat_id": "string",
                "fdt": "string", "notes": "string"
            }
            valid_astype_dict_user = {}
            missing_cols_user = []
            for col, dtype in astype_dict_user.items():
                 if col in user_data.columns:
                     valid_astype_dict_user[col] = dtype
                 else:
                     missing_cols_user.append(col)

            if missing_cols_user:
                 print(f"    ⚠️ Kolom berikut tidak ditemukan di user_data dan dilewati dalam konversi tipe data: {missing_cols_user}")

            if valid_astype_dict_user:
                 user_data = user_data.astype(valid_astype_dict_user)
                 print("  ✅ Konversi tipe data user_data selesai.")
            else:
                 print("  ℹ️ Tidak ada kolom di user_data yang perlu dikonversi via astype.")

        except Exception as e:
            print(f"❌ Error saat konversi tipe data user_data: {e}")

    elif 'user_data' in locals() and isinstance(user_data, pd.DataFrame) and user_data.empty:
          print("\n⚠️ DataFrame user_data kosong, konversi tipe data dilewati.")

except NameError:
    print("\nℹ️ Variabel user_data tidak terdefinisi. Konversi tipe data untuk user_data dilewati.")


# ========================= Cek hasil =========================
# ... (bagian kode print info dari jawaban sebelumnya) ...
print("\n📋 Info aset_data setelah konversi:")
if 'aset_data' in locals() and isinstance(aset_data, pd.DataFrame):
    if not aset_data.empty:
        aset_data.info()
        # Opsional: Print tipe data spesifik untuk kolom-kolom kunci yang diproses
        key_cols_to_check = ['tanggal_rfs', 'kapasitas_olt', 'kapasitas_port_olt', 'olt_port', 'jumlah_splitter_fdt', 'hc_old']
        for col in key_cols_to_check:
            if col in aset_data.columns:
                 print(f"   -> Tipe data kolom '{col}': {aset_data[col].dtype}")
    else:
        print("   (DataFrame kosong)")
else:
    print("   (DataFrame aset_data tidak terdefinisi)")


print("\n📋 Info user_data setelah konversi:")
try:
    if 'user_data' in locals() and isinstance(user_data, pd.DataFrame):
        if not user_data.empty:
            user_data.info()
        else:
            print("   (DataFrame kosong)")
except NameError:
    print("   (DataFrame user_data tidak terdefinisi)")


print("\n✅ Proses konversi tipe data selesai.")

import pandas as pd
import re
import numpy as np # Import numpy untuk np.nan
# from decimal import Decimal # Asumsi ini tidak diperlukan jika hanya konversi ke float

# ========================= Cleansing Functions =========================

def clean_degree_with_comma(coord):
    if pd.isna(coord) or 'Â,' not in str(coord):
        return None
    coord = str(coord).replace('Â', '')
    # Setelah hapus Â, mungkin format jadi lat,lon. Coba standardisasi dengan clean_comma_separated
    return clean_comma_separated(coord) # Rantai ke fungsi lain yang lebih umum

def clean_degree_separated(coord):
    if pd.isna(coord):
        return None
    coord_str = str(coord)
    if 'Â°' not in coord_str:
        return None
    try:
        # Contoh: -7.361902Â°112.693948Â°
        # Temukan Â° pertama, pisahkan. Buang semua Â°. Sisipkan koma.
        parts = coord_str.split('Â°')
        if len(parts) >= 2:
             lat_part = parts[0].strip()
             lon_part = parts[1].strip().replace('Â°','') # Hapus Â° sisa di lon
             # Coba standardisasi dengan clean_comma_separated (jika lat/lon masih ada koma/titik desimal beda)
             return clean_comma_separated(f"{lat_part},{lon_part}")
    except:
         pass
    return None

def clean_degree_as_separator(coord):
    if pd.isna(coord):
        return None
    coord_str = str(coord)
    if '°' not in coord_str:
        return None
    try:
        # Contoh: -6.9271° 107.6048°
        # Temukan ° pertama, pisahkan. Buang semua °. Sisipkan koma.
        parts = coord_str.split('°')
        if len(parts) >= 2:
            lat_part = parts[0].strip()
            lon_part = ''.join(parts[1:]).strip() # Gabungkan sisa part jika ada banyak °, strip
            # Coba standardisasi dengan clean_comma_separated
            return clean_comma_separated(f"{lat_part},{lon_part}")
    except:
         pass
    return None

def clean_two_commas_with_space(coord):
    if pd.isna(coord):
        return None
    coord_str = str(coord).strip()
    # Contoh: -8,1948403, 111,1077904 atau -7,2892906 112,7276532
    # Pola: AngkaDesimalKoma Spasi AngkaDesimalKoma ATAU AngkaDesimalKoma, Spasi AngkaDesimalKoma
    # Kita bisa gabungkan dan standardisasi dengan clean_comma_separated
    coord_str_standardized = coord_str.replace(', ', ',').replace(' ', ',') # Ganti ' ' atau ', ' jadi ','
    return clean_comma_separated(coord_str_standardized)


def clean_comma_separated(coord):
    """Handles standard 'lat,lon' format and standardizes decimal comma/dot."""
    if pd.isna(coord):
        return None
    coord_str = str(coord).strip()
    if ',' not in coord_str: # Harus ada setidaknya 1 koma sebagai pemisah
         return None
    parts = [part.strip() for part in coord_str.split(',', 1)] # Split hanya pada koma pertama
    if len(parts) == 2:
        lat_part = parts[0].replace(',', '.') # Standardisasi: desimal selalu titik
        lon_part = parts[1].replace(',', '.') # Standardisasi: desimal selalu titik
        # Cek apakah kedua part terlihat seperti angka setelah standardisasi
        try:
            float(lat_part)
            float(lon_part)
            return f"{lat_part},{lon_part}" # Kembalikan format standar "lat,lon"
        except ValueError:
             return None # Jika tidak bisa diubah jadi float, format salah
    return None # Jika split tidak menghasilkan 2 part


# Tambahkan fungsi baru untuk format titik-spasi
def clean_dot_space_separated(coord):
    """Handles format like '-7.90845. 113.35127' -> '-7.90845,113.35127'"""
    if pd.isna(coord):
        return None
    coord_str = str(coord).strip()
    if coord_str == '': return None

    # Regex to match format like Number. Space(s) Number
    # Allows optional minus at start, numbers, optional dot, numbers after dot.
    # Captures the two numeric parts separated by dot and one or more spaces.
    match = re.match(r'^(-?\d+\.?\d*)\.\s+(\-?\d+\.?\d*)$', coord_str)
    if match:
        lat_part = match.group(1)
        lon_part = match.group(2)
        # Parts should already use '.' as decimal due to regex.
        return f"{lat_part},{lon_part}"

    return None


def clean_dot_separated_no_comma(coord):
    """Handles format like 'X.Y.A.B' or 'X.YYYYYYY.AAAAAAA' -> 'X.Y,A.B'"""
    if pd.isna(coord): return None
    coord_str = str(coord).strip()
    if ',' in coord_str: return None # Jangan proses jika ada koma

    # Coba regex yang mencari pola Angka.Angka.Angka.Angka atau Angka.Angka.Angka
    # Regex: Angka (opsional -) diikuti . diikuti Angka, kemudian . Angka (opsional . Angka lagi di akhir)
    # Ini mencoba menangani beberapa pola titik sebagai pemisah utama
    match = re.match(r'^(-?\d+\.?\d*)\.(\d+\.?\d*)\.?(\d*\.?\d*)$', coord_str)
    if match:
        # Jika match, kita asumsikan split utamanya adalah setelah titik pertama
        lat_part = match.group(1) # Ini harusnya Lat integer + desimal
        # Gabungkan group 2 dan 3 sebagai Lon
        lon_part = match.group(2) # Ini harusnya Lon integer + desimal
        if match.group(3): # Jika ada group ke-3 (dari pola X.Y.A.B.C)
             # Pola X.Y.A.B.C mungkin A.B lat, C.D lon
             # Atau X.Y lat, A.B.C lon -> ini rumit
             # Berdasarkan contoh "-7.36271456342.732918" -> "-7.362714,112.732918"
             # Ini bukan pola A.B.C.D.
             # Mari fokus pada pola X.Y.A.B -> X.Y,A.B
             # Asumsi regex ^(-?\d+\.?\d*)\.(\d+\.?\d*)$ lebih relevan untuk pola dua bagian dipisah titik
             # Kita pakai regex spesifik ^(-?\d+\.\d+)\.(\d+\.?\d+)$ seperti di clean_dot_space_separated tapi tanpa spasi
             match_simple_dot_sep = re.match(r'^(-?\d+\.\d+)\.(\d+\.?\d*)$', coord_str)
             if match_simple_dot_sep:
                 lat_part = match_simple_dot_sep.group(1)
                 lon_part = match_simple_dot_sep.group(2)
                 return clean_comma_separated(f"{lat_part},{lon_part}") # Standardisasi via clean_comma_separated


    return None # Jika tidak cocok pola titik sebagai pemisah utama tanpa koma


def clean_merged_coordinates(coord):
    """Handles formats like -7.36271456342.732918 (jika ada pola khusus) atau -8180339111.116929"""
    if pd.isna(coord): return None
    coord_str = str(coord).strip().replace(" ", "").replace(",", "") # Hapus spasi dan koma

    # Contoh: -8180339111.116929
    # Regex: mulai negatif opsional, digit (lat integer+decimal), digit (lon integer), titik (decimal), digit (lon decimal)
    # Pola di regex asli: ^(-?\d+\.\d+)(\d{3}\.\d+)$ cocok untuk A.B C.D dimana C 3 digit.
    # Untuk -8180339111.116929, format ini mungkin dari konversi float besar.
    # Pola raw_data -8180339111.116929 -> harus jadi -8.180339,111.116929
    # Ini diproses oleh clean_split_from_long_float jika inputnya float/int.
    # Jika inputnya string "-8180339111.116929", clean_split_from_long_float akan return None.
    # Regex yang mungkin cocok string ini: ^(-?\d+)(\d{3}\.?\d*)\.(\d+\.?\d*)$
    match_big_number = re.match(r'^(-?\d+)(\d{3}\.?\d*)\.(\d+\.?\d*)$', coord_str)
    if match_big_number:
         # Ini mungkin bukan format lat,lon yang sebenarnya.
         # Pola -8180339111.116929 => -8 (lat int) 180339 (lat dec) 111 (lon int) 116929 (lon dec)
         # Regex: ^(-?\d)(\d+)(\d{3})\.(\d+)$
         match_specific_merged = re.match(r'^(-?\d)(\d+)(\d{3})\.(\d+)$', coord_str)
         if match_specific_merged:
              lat_int = match_specific_merged.group(1)
              lat_dec = match_specific_merged.group(2)
              lon_int = match_specific_merged.group(3)
              lon_dec = match_specific_merged.group(4)
              # Format menjadi -LatInt.LatDec,LonInt.LonDec
              return f"{lat_int}.{lat_dec},{lon_int}.{lon_dec}"
         # Jika tidak cocok pola spesifik ini, biarkan regex asli jika masih relevan
         match = re.match(r'^(-?\d+\.\d+)(\d{3}\.\d+)$', coord_str) # Regex asli
         if match:
             lat = match.group(1)
             lon = match.group(2)
             return f"{lat},{lon}" # Kembalikan format standar lat,lon

    return None # Jika tidak cocok pola gabungan


def move_comma_separator(coord):
    # Ini terlihat seperti logika kompleks untuk format spesifik, biarkan seperti adanya.
    if pd.isna(coord) or ',' not in str(coord) or str(coord).count('.') != 2:
        return None
    try:
        # Contoh: 7.123.456,112.789.012 -> 7.123456,112.789012 (ini asumsi saya)
        # Kode asli Anda sepertinya memindahkan bagian setelah desimal pertama ke Lon
        # lat_part, lon_part = str(coord).split(',')
        # lat_before, lat_after = lat_part.split('.') # lat_before='7', lat_after='123.456' -> error di sini
        # Ini logicnya perlu disesuaikan dengan format persisnya.
        # Biarkan fungsi ini seperti aslinya jika memang menangani format spesifik.
        # Jika contoh raw_data "8.180339,111.116929" ini yang dimaksud, itu ditangani clean_comma_separated
        # Jika contoh "7.123.456,112.789.012" yang dimaksud, regex di clean_dot_separated_no_comma mungkin lebih pas jika koma dihilangkan
        pass # Biarkan logika asli atau kembalikan None jika tidak yakin
    except:
        pass
    return None # Kembalikan None karena logikanya rumit dan rentan error

def clean_with_e_separator(coord):
    # Ini terlihat sudah menangani format E, biarkan seperti adanya.
    if pd.isna(coord): return None
    coord_str = str(coord).strip()
    if 'E' not in coord_str: return None
    try:
        # Contoh: -7.86482E112.69473 atau S709.168E11224.693
        # Split by 'E'
        parts = coord_str.split('E')
        if len(parts) == 2:
            lat_part = parts[0].strip()
            lon_part = parts[1].strip()

            # Tangani S/N di Latitude
            is_south = False
            if lat_part.upper().startswith('S'):
                 is_south = True
                 lat_part = lat_part[1:].strip() # Hapus 'S'

            # Asumsi format sebelum E adalah angka desimal (X.Y) atau integer (X)
            # Asumsi format setelah E adalah angka desimal (A.B) atau integer (A)
            # Konversi ke float untuk validasi dan standardisasi (jika perlu penyesuaian skala seperti di kode asli)
            try:
                 lat_float = float(lat_part)
                 lon_float = float(lon_part)

                 # Jika ada logika penyesuaian skala seperti "/ 100" di kode asli, terapkan di sini
                 # Contoh: S709.168 -> 7.09168 (geser koma 2 tempat)
                 # Asumsi 709.168E11224.693 -> 7.09168, 112.24693 (geser koma 2 tempat untuk lat dan 3 untuk lon?)
                 # Logika asli Anda: lat/100, lon/100. Mari pertahankan jika itu intentnya.
                 # Ini mungkin tergantung format persis sebelum E.
                 # Mari kembalikan nilai float, lalu biarkan konversi akhir menangani float ke string.
                 # Atau kembalikan string "lat,lon" dengan float format
                 lat_final = lat_float / 100 if abs(lat_float) > 90 else lat_float # Contoh heuristic: jika >90, geser koma
                 lon_final = lon_float / 100 if abs(lon_float) > 180 else lon_float # Contoh heuristic: jika >180, geser koma
                 # Logika asli Anda hanya bagi 100. Mari pertahankan jika itu polanya.
                 lat_final = lat_float / 100
                 lon_final = lon_float / 100

                 if is_south:
                     lat_final = -abs(lat_final) # Pastikan negatif jika 'S'

                 # Kembalikan sebagai string "lat,lon"
                 return f"{lat_final},{lon_final}"

            except ValueError:
                 return None # Jika partnya bukan angka

    except:
         pass
    return None


def clean_split_from_long_float(coord):
    """Handles large raw float/int numbers like -8180339111.116929 -> -8.180339,111.116929"""
    # Ini adalah kasus spesifik jika data awalnya berupa angka float atau int yang besar
    try:
        # Cek apakah input adalah float atau int, BUKAN string
        if not isinstance(coord, (float, int)) or pd.isna(coord):
            return None

        # Ubah angka menjadi string dengan presisi tinggi, buang titik desimal, buang minus di awal jika ada
        # Contoh: -8180339111.116929 -> "-8180339111.11692900000000" -> "818033911111692900000000"
        # Sesuaikan format presisi sesuai data Anda
        coord_str_raw = "{:.10f}".format(coord).replace('.', '').replace('-', '')
        # Hapus nol di belakang jika ada (misal .000)
        coord_str = coord_str_raw.rstrip('0')

        if len(coord_str) < 10: # Heuristic: Asumsi lat+lon punya total digit minimal segini
             return None # Angka terlalu kecil untuk pola gabungan ini

        # Pola: -LatInt LatDec LonInt LonDec
        # -8.180339,111.116929
        # Lat: -8 (1 digit) . 180339 (6 digit)
        # Lon: 111 (3 digit) . 116929 (6 digit)
        # Total digit (tanpa minus dan titik): 1 + 6 + 3 + 6 = 16 digit
        # Jika pola selalu LatInt(1).LatDec(6)LonInt(3).LonDec(>=1)
        # Cari 6 digit setelah digit pertama (integer lat) sebagai lat desimal
        # Cari 3 digit setelah lat desimal sebagai lon integer
        # Sisanya adalah lon desimal
        # string gabungan: -8 180339 111 116929
        # index: 0  1..6   7..9 10..akhir
        # Lengths: 1, 6, 3, ?
        # Check panjang string: minimal 10 (1+6+3)
        if len(coord_str) < 10:
             return None

        try:
             lat_int_digit = coord_str[0] # Asumsi 1 digit integer lat (setelah hapus minus)
             lat_dec_digits = coord_str[1:7] # Asumsi 6 digit desimal lat
             lon_int_digits = coord_str[7:10] # Asumsi 3 digit integer lon
             lon_dec_digits = coord_str[10:] # Sisa adalah desimal lon

             # Rekonstruksi string "lat,lon"
             lat = f"-{lat_int_digit}.{lat_dec_digits}" # Tambahkan kembali minus jika awal negatif
             lon = f"{lon_int_digits}.{lon_dec_digits}"

             # Cek apakah angka asli negatif
             if coord < 0:
                  lat = f"-{lat_int_digit}.{lat_dec_digits}" # Tambahkan kembali minus

             # Final standardisasi dan validasi
             return clean_comma_separated(f"{lat},{lon}")

        except IndexError:
             return None # String tidak cukup panjang untuk pola


    except:
        # Tangani error lain jika format float/int tidak seperti yang diharapkan
        pass # Fall through

    return None # Jika input bukan float/int atau pola tidak cocok


# --- Fungsi clean_comma_separated yang Dimodifikasi ---
def clean_comma_separated(coord):
    """
    Handles standard 'lat,lon' format, standardizes decimal comma/dot,
    and removes multiple consecutive dots (e.g., '..').
    """
    if pd.isna(coord):
        return None
    coord_str = str(coord).strip()
    if ',' not in coord_str: # Harus ada setidaknya 1 koma sebagai pemisah
         return None
    # Split hanya pada koma pertama untuk memisahkan lat dan lon
    parts = [part.strip() for part in coord_str.split(',', 1)]
    if len(parts) == 2:
        lat_part = parts[0].replace(',', '.') # Standardisasi: desimal selalu titik
        lon_part = parts[1].replace(',', '.') # Standardisasi: desimal selalu titik

        # --- PENTING: Hapus titik berurutan di sini ---
        # Ganti satu atau lebih titik berurutan (\.+) dengan satu titik (.)
        lat_part = re.sub(r'\.+', '.', lat_part)
        lon_part = re.sub(r'\.+', '.', lon_part)

        # Pastikan tidak ada titik di awal atau akhir setelah penggantian
        lat_part = lat_part.strip('.')
        lon_part = lon_part.strip('.')
        # --- End of Added Step ---

        # Cek apakah kedua part terlihat seperti angka setelah standardisasi dan pembersihan titik
        try:
            float(lat_part)
            float(lon_part)
            return f"{lat_part},{lon_part}" # Kembalikan format standar "lat,lon" string
        except ValueError:
             return None # Jika tidak bisa diubah jadi float, format salah
    return None # Jika split tidak menghasilkan 2 part

# --- Fungsi apply_cleaning (Tetap sama, karena memanggil clean_comma_separated) ---
def apply_cleaning(coord):
    """
    Proses data latitude/longitude untuk numerik dan string,
    mencoba berbagai format pembersihan, mengembalikan string format "lat,lon" atau None.
    """
    # Langkah 1: Tangani input None/NaN langsung
    if pd.isna(coord):
        return None

    # Langkah 2: Coba format yang inputnya mungkin berupa angka mentah (float/int)
    cleaned_coord = clean_split_from_long_float(coord)
    if cleaned_coord is not None:
        return cleaned_coord

    # Langkah 3: Jika input bukan angka atau clean_split_from_long_float gagal, coba sebagai string
    coord_str_raw = str(coord)
    # ** PENTING: Hapus karakter problematic (Â, NBSP) di awal **
    coord_str = coord_str_raw.replace('Â', '').replace('\u00A0', ' ').strip()

    # Tangani string kosong atau representasi null dalam string setelah pembersihan awal
    if coord_str == '' or coord_str.lower() in ['none', 'nan', '<na>']:
        return None

    # Coba fungsi pembersihan berbasis string dalam urutan yang logis
    # Fungsi yang lebih spesifik atau yang menangani pemisah non-standar didahulukan
    cleaned_coord = (
        # clean_degree_with_comma(coord_str) # Dihapus
        # clean_degree_separated(coord_str) # Dihapus
        clean_degree_as_separator(coord_str) or # Ex: -6.9271° 107.6048°
        clean_two_commas_with_space(coord_str) or # Ex: -8,1948403, 111,1077904
        clean_dot_space_separated(coord_str) or # Ex: -7.90845. 113.35127
        clean_with_e_separator(coord_str) or # Ex: -7.86482E112.69473
        clean_dot_separated_no_comma(coord_str) or # Ex: -7.90845.113.35127
        clean_merged_coordinates(coord_str) or # Ex: -7.362714563.427329
        move_comma_separator(coord_str) # Ex: 7.123.456,112.789.012
        # clean_comma_separated tidak dipanggil di sini agar hanya menjadi fallback
    )

    if cleaned_coord is not None:
        # Jika salah satu fungsi pembersih spesifik berhasil, ia mengembalikan string "lat,lon"
        return cleaned_coord

    # Langkah 4: Fallback terakhir - coba split koma jika ada koma
    # Ini menangani format standar "lat,lon" atau kasus lain yang setelah pembersihan awal
    # menghasilkan string dengan koma sebagai pemisah utama (termasuk "-7.331868..,112.637920").
    if ',' in coord_str:
        # Gunakan logika clean_comma_separated (yang sudah dimodifikasi)
        return clean_comma_separated(coord_str)


    # Langkah 5: Jika tidak ada format yang cocok sama sekali
    return None


# ========================= Contoh Data (Tambahkan kasus baru) =========================
raw_data = [
    "-7.361902Â,112.693948Â", # Ini data asli, setelah Â dihapus -> "-7.361902,112.693948"
    "-7.361902Â°112.693948Â°", # setelah Â dihapus -> "-7.361902°112.693948°"
    "-6.9271° 107.6048°",
    "-8,1948403, 111,1077904",
    "8.180339,111.116929",
    "-7,2892906 112,7276532",
    "-7.36271456342.732918",
    -8180339111.116929,
    "-7.86482E112.69473",
    "S709.168E11224.693",
    "-7.90845. 113.35127",
    "  -5.123 . 110.456 ",
    "1.2.3.4.5",
    "abc",
    "123",
    "123,abc",
    "123.45,abc.def",
    None, np.nan, "", " ", "<NA>",
    # --- Kasus baru yang ingin ditangani ---
    "-7.331868..,112.637920", # Kasus double dot
    "-7.331868...,112.637920", # Kasus triple dot
    "-7..331868,112.637920", # Kasus double dot di tempat lain (setelah minus)
    "8.123..,110.456..", # Kasus double dot di kedua sisi
    "1...2...3,4...5...6" # Kasus banyak titik di kedua sisi
]

# ========================= Proses Data =========================
print("Memproses data contoh...")
cleaned_data = [apply_cleaning(coord) for coord in raw_data]

# ========================= Output =========================
print("\nOriginal -> Cleaned (string format 'lat,lon' atau None)")
print("=" * 60)
for i, (original, cleaned) in enumerate(zip(raw_data, cleaned_data), 1):
    # Gunakan repr() atau !r untuk menampilkan string secara literal (termasuk '', None)
    print(f"{i}. Original: {original!r}")
    print(f"   ✅ Cleaned : {cleaned!r}")
    # Opsional: Cek apakah hasil cleaned bisa dipecah dan diubah jadi float
    if isinstance(cleaned, str) and ',' in cleaned:
        try:
            lat, lon = cleaned.split(',', 1)
            lat_float = float(lat)
            lon_float = float(lon)
            print(f"   -> Hasil OK, siap konversi Float. Lat: {lat_float}, Lon: {lon_float}")
        except (ValueError, TypeError):
            print("   !!! Peringatan: Hasil clean tidak bisa dikonversi jadi Float! !!!")
    elif cleaned is None:
         print("   -> Hasil adalah None (tidak dikenali).")
    else:
         print(f"   !!! Peringatan: Hasil clean bukan string 'lat,lon' atau None! ({type(cleaned)}) !!!")

print("=" * 60)

def clean_invalid_characters(value):
    """
    Membersihkan simbol atau karakter tidak valid dari nilai latitude/longitude
    tanpa membuatnya kosong atau NaN.
    """
    if pd.isna(value):
        return value
    try:
        return re.sub(r'[^\d\.,-]', '', str(value))
    except Exception:
        return value

  # Validasi dan split ulang latitude_olt menjadi latitude_olt_clean dan longitude_olt_clean
def validate_and_split(value):
    """
    Memastikan bahwa data latitude_olt memiliki koma (,) untuk split.
    Jika tidak, biarkan nilai original di latitude dan None untuk longitude.
    """
    if pd.isna(value) or ',' not in value:
        return pd.Series([value, None])  # Kembalikan latitude dan kosongkan longitude
    try:
        lat, lon = value.split(",", 1)
        return pd.Series([lat.strip(), lon.strip()])  # Bersihkan spasi tambahan
    except ValueError:
        return pd.Series([value, None])  # Jika error, biarkan nilai original

print("\n========================= Cleansing Kolom Koordinat =========================\n")

# Asumsi kolom 'koordinat_olt' ada
if 'koordinat_olt' in aset_data.columns:
    print("ℹ️ Memproses kolom 'koordinat_olt' (gabungan Latitude, Longitude)...")
    try:
        # Langkah 1: Apply initial cleaning to the combined coordinate string
        # apply_cleaning harus mengembalikan string atau nilai yang bisa di-astype(str)
        # Kembalikan '' atau None/np.nan untuk nilai yang tidak valid
        aset_data["koordinat_olt_cleaned"] = aset_data["koordinat_olt"].apply(apply_cleaning)
        print("✅ apply_cleaning pada 'koordinat_olt' selesai.")

        # Langkah 2: Split cleaned coordinate string into two new columns
        # Gunakan .str.split(',', expand=True) untuk memecah string menjadi DataFrame 2 kolom
        # Jika koordinat_olt_cleaned adalah None/NaN, .str.split akan menghasilkan NaN di kedua kolom.
        # Jika koordinat_olt_cleaned adalah '', .str.split(',', expand=True) akan menghasilkan DataFrame dengan ['',''].
        # Jika koordinat_olt_cleaned adalah 'hanya_lat' atau 'hanya_lon,', split akan menghasilkan 1 atau 2 kolom dengan ''/None
        # Kita akan menugaskannya ke kolom target dan menanganinya di langkah pembersihan berikutnya.
        split_coords = aset_data["koordinat_olt_cleaned"].str.split(',', expand=True)
        print("✅ Split 'koordinat_olt_cleaned' selesai.")

        # Langkah 3: Tugaskan hasil split ke kolom target latitude_olt dan longitude_olt
        # Method .str.split(expand=True) mengembalikan DataFrame baru.
        # Kolom pertama hasil split adalah index 0, kolom kedua adalah index 1.
        # Jika split menghasilkan kurang dari 2 kolom (misal input 'hanya_lat'),
        # mengakses split_coords[1] akan menghasilkan kolom yang semuanya NaN.
        # Jika split menghasilkan lebih dari 2 kolom (misal input 'a,b,c'), kolom sisanya akan diabaikan di sini.

        # Inisialisasi kolom target dengan NaN jika belum ada atau untuk menimpa data lama
        aset_data["latitude_olt"] = np.nan
        aset_data["longitude_olt"] = np.nan

        # Tugaskan kolom hasil split jika split_coords memiliki minimal 1 kolom
        if split_coords.shape[1] > 0:
             aset_data["latitude_olt"] = split_coords[0]
             print("✅ Kolom 0 hasil split ditugaskan ke latitude_olt.")

        # Tugaskan kolom hasil split jika split_coords memiliki minimal 2 kolom
        if split_coords.shape[1] > 1:
             aset_data["longitude_olt"] = split_coords[1]
             print("✅ Kolom 1 hasil split ditugaskan ke longitude_olt.")
        else:
             print("⚠️ Split tidak menghasilkan 2 kolom, longitude_olt mungkin diisi NaN atau tidak berubah.")


        # Langkah 4: Lakukan pembersihan dan konversi akhir pada latitude_olt dan longitude_olt
        # Nilai di sini adalah string (atau NaN jika split gagal atau input awal null)
        # Mereka mungkin perlu pembersihan karakter, standardisasi desimal, dan konversi ke float

        print("ℹ️ Melakukan pembersihan dan konversi akhir pada latitude_olt dan longitude_olt.")

        # Pastikan kolom target adalah string untuk operasi selanjutnya
        # Ini penting untuk menangani np.nan -> '<NA>' atau nilai lain menjadi string
        aset_data["latitude_olt"] = aset_data["latitude_olt"].astype(str)
        aset_data["longitude_olt"] = aset_data["longitude_olt"].astype(str)
        print("✅ latitude_olt dan longitude_olt diubah ke string.")

        # Tangani string representasi null yang mungkin ada setelah split dan astype(str)
        # Termasuk '', '<NA>', 'None', dan string kosong setelah strip
        aset_data["latitude_olt"] = aset_data["latitude_olt"].replace(['', '<NA>', 'None'], np.nan).str.strip()
        aset_data["longitude_olt"] = aset_data["longitude_olt"].replace(['', '<NA>', 'None'], np.nan).str.strip()
        print("✅ String kosong/'<NA>'/'None'/spasi di-replace dengan NaN/dihapus.")

        # Apply clean_invalid_characters jika dibutuhkan. Fungsi ini harus menangani string atau NaN.
        # Perannya adalah membersihkan karakter *di dalam* string yang seharusnya berupa angka float.
        # Pastikan clean_invalid_characters(np.nan) mengembalikan np.nan.
        aset_data["latitude_olt"] = aset_data["latitude_olt"].apply(clean_invalid_characters)
        aset_data["longitude_olt"] = aset_data["longitude_olt"].apply(clean_invalid_characters)
        print("✅ clean_invalid_characters selesai pada latitude_olt dan longitude_olt.")

        # Final conversion to float, coercing any remaining errors to NaN
        # pd.to_numeric adalah cara paling tangguh untuk ini.
        aset_data['latitude_olt'] = pd.to_numeric(aset_data['latitude_olt'], errors='coerce')
        aset_data['longitude_olt'] = pd.to_numeric(aset_data['longitude_olt'], errors='coerce')
        print("✅ Konversi akhir ke float selesai pada latitude_olt dan longitude_olt.")

        # Hitung berapa banyak nilai non-null awal di koordinat_olt yang menghasilkan NaN di lat/lon
        # Hitung jumlah NaN di kolom lat/lon hasil akhir
        initial_coord_non_null = aset_data["koordinat_olt"].notna().sum()
        final_lat_null_count = aset_data["latitude_olt"].isna().sum()
        final_lon_null_count = aset_data["longitude_olt"].isna().sum()
        # Jumlah nilai yang *awalnya* di koordinat_olt tapi *akhirnya* jadi NaN di lat/lon (minimal salah satu)
        # Ini agak kompleks dihitung tepat, tapi jumlah NaN akhir memberikan gambaran.
        print(f"ℹ️ Total baris: {len(aset_data)}")
        print(f"ℹ️ Baris 'koordinat_olt' non-kosong awal: {initial_coord_non_null}")
        print(f"ℹ️ Baris 'latitude_olt' yang menjadi kosong (NaN): {final_lat_null_count}")
        print(f"ℹ️ Baris 'longitude_olt' yang menjadi kosong (NaN): {final_lon_null_count}")


        # Drop the intermediate cleaned coordinate column if no longer needed
        if "koordinat_olt_cleaned" in aset_data.columns:
             aset_data.drop(columns=["koordinat_olt_cleaned","koordinat_olt"], inplace=True)

             print("✅ Kolom 'koordinat_olt_cleaned' dihapus.")


    except Exception as e:
        print(f"❌ Error dalam proses cleansing koordinat OLT: {e}")
        # Jika terjadi error di sini, pastikan latitude_olt dan longitude_olt diisi NaN
        # agar tidak error di langkah selanjutnya yang mungkin mengasumsikan kolom ini ada.
        if "latitude_olt" not in aset_data.columns: aset_data["latitude_olt"] = np.nan
        if "longitude_olt" not in aset_data.columns: aset_data["longitude_olt"] = np.nan
        print("⚠️ Kolom latitude_olt dan longitude_olt mungkin tidak terproses sepenuhnya karena error.")


else:
    print("⚠️ Kolom 'koordinat_olt' tidak ditemukan dalam aset_data. Melewatkan cleansing koordinat OLT.")

# Output informasi
print("\n📋 Info aset_data setelah cleansing koordinat:")
aset_data.info()

# Mencetak info hanya untuk kolom tertentu
if 'latitude_olt' in aset_data.columns and 'longitude_olt' in aset_data.columns:
    print("\n📋 Info kolom 'latitude_olt' dan 'longitude_olt' setelah cleansing:")
    print(aset_data[['latitude_olt', 'longitude_olt']].info())
else:
    print("\n⚠️ Kolom 'latitude_olt' atau 'longitude_olt' tidak ditemukan setelah cleansing.")

aset_data.to_csv("data_clean/aset_data_clean.csv", index=False)

def fix_latitude(value):
    if pd.isna(value):
        return np.nan
    value = str(value).replace(" ", "").replace(",", ".")
    match = re.match(r"(-?\d+)\.(\d+)", value)
    if not match:
        return np.nan
    pre, post = match.groups()

    if len(pre.replace("-", "")) > 2:
        # contoh: -727.037 → -7.27037
        new_pre = pre[:-2]  # ambil dua angka terakhir jadi bagian depan desimal
        new_post = pre[-2:] + post
        return float(f"{new_pre}.{new_post}")
    return float(value)

def fix_longitude(value):
    if pd.isna(value):
        return np.nan
    value = str(value).replace(" ", "").replace(",", ".")
    match = re.match(r"(-?\d+)\.(\d+)", value)
    if not match:
        return np.nan
    pre, post = match.groups()

    if len(pre) > 3:
        # contoh: 11233.925 → 112.33925
        new_pre = pre[:-3]
        new_post = pre[-3:] + post
        return float(f"{new_pre}.{new_post}")
    return float(value)

print("\n========================= Cleansing Kolom Koordinat =========================\n")

# Asumsi kolom 'koordinat_fdt' ada
if 'koordinat_fdt' in aset_data.columns:
    print("ℹ️ Memproses kolom 'koordinat_fdt' (gabungan Latitude, Longitude)...")
    try:
        # Langkah 1: Pembersihan awal pada string koordinat
        aset_data["koordinat_fdt_cleaned"] = aset_data["koordinat_fdt"].apply(apply_cleaning)
        print("✅ apply_cleaning pada 'koordinat_fdt' selesai.")

        # Langkah 2: Pisahkan string koordinat menjadi Latitude dan Longitude
        split_coords = aset_data["koordinat_fdt_cleaned"].str.split(',', expand=True)
        print("✅ Split 'koordinat_fdt_cleaned' selesai.")

        # Langkah 3: Inisialisasi kolom latitude_fdt dan longitude_fdt dengan NaN
        aset_data["latitude_fdt"] = np.nan
        aset_data["longitude_fdt"] = np.nan

        # Tugaskan hasil split ke kolom target
        if split_coords.shape[1] > 0:
            aset_data["latitude_fdt"] = split_coords[0]
            print("✅ Kolom 0 hasil split ditugaskan ke latitude_fdt.")

        if split_coords.shape[1] > 1:
            aset_data["longitude_fdt"] = split_coords[1]
            print("✅ Kolom 1 hasil split ditugaskan ke longitude_fdt.")
        else:
            print("⚠️ Split tidak menghasilkan 2 kolom, longitude_fdt mungkin diisi NaN atau tidak berubah.")

        # Langkah 4: Pembersihan dan konversi akhir
        print("ℹ️ Melakukan pembersihan dan konversi akhir pada latitude_fdt dan longitude_fdt.")
        aset_data["latitude_fdt"] = aset_data["latitude_fdt"].astype(str)
        aset_data["longitude_fdt"] = aset_data["longitude_fdt"].astype(str)
        print("✅ latitude_fdt dan longitude_fdt diubah ke string.")

        aset_data["latitude_fdt"] = aset_data["latitude_fdt"].replace(['', '<NA>', 'None'], np.nan).str.strip()
        aset_data["longitude_fdt"] = aset_data["longitude_fdt"].replace(['', '<NA>', 'None'], np.nan).str.strip()
        print("✅ String kosong/'<NA>'/'None'/spasi di-replace dengan NaN/dihapus.")

        aset_data["latitude_fdt"] = aset_data["latitude_fdt"].apply(clean_invalid_characters)
        aset_data["longitude_fdt"] = aset_data["longitude_fdt"].apply(clean_invalid_characters)
        print("✅ clean_invalid_characters selesai pada latitude_fdt dan longitude_fdt.")

        aset_data["latitude_fdt"] = pd.to_numeric(aset_data["latitude_fdt"], errors="coerce")
        aset_data["longitude_fdt"] = pd.to_numeric(aset_data["longitude_fdt"], errors="coerce")
        print("✅ Konversi akhir ke float selesai pada latitude_fdt dan longitude_fdt.")

        # Hitung jumlah baris non-null awal dan setelah proses
        initial_coord_non_null = aset_data["koordinat_fdt"].notna().sum()
        final_lat_null_count = aset_data["latitude_fdt"].isna().sum()
        final_lon_null_count = aset_data["longitude_fdt"].isna().sum()
        print(f"ℹ️ Total baris: {len(aset_data)}")
        print(f"ℹ️ Baris 'koordinat_fdt' non-kosong awal: {initial_coord_non_null}")
        print(f"ℹ️ Baris 'latitude_fdt' yang menjadi kosong (NaN): {final_lat_null_count}")
        print(f"ℹ️ Baris 'longitude_fdt' yang menjadi kosong (NaN): {final_lon_null_count}")

        # Drop kolom sementara jika tidak lagi dibutuhkan
        if "koordinat_fdt_cleaned" in aset_data.columns:
            aset_data.drop(columns=["koordinat_fdt_cleaned", "koordinat_fdt"], inplace=True)
            print("✅ Kolom 'koordinat_fdt_cleaned' dihapus.")

    except Exception as e:
        print(f"❌ Error dalam proses cleansing koordinat FDT: {e}")
        if "latitude_fdt" not in aset_data.columns:
            aset_data["latitude_fdt"] = np.nan
        if "longitude_fdt" not in aset_data.columns:
            aset_data["longitude_fdt"] = np.nan
        print("⚠️ Kolom latitude_fdt dan longitude_fdt mungkin tidak terproses sepenuhnya karena error.")

else:
    print("⚠️ Kolom 'koordinat_fdt' tidak ditemukan dalam aset_data. Melewatkan cleansing koordinat FDT.")

# Output informasi
print("\n📋 Info aset_data setelah cleansing koordinat:")
aset_data.info()

if "latitude_fdt" in aset_data.columns and "longitude_fdt" in aset_data.columns:
    print("\n📋 Info kolom 'latitude_fdt' dan 'longitude_fdt' setelah cleansing:")
    print(aset_data[["latitude_fdt", "longitude_fdt"]].info())
else:
    print("\n⚠️ Kolom 'latitude_fdt' atau 'longitude_fdt' tidak ditemukan setelah cleansing.")

print("\n========================= Cleansing Kolom Koordinat =========================\n")

# Asumsi kolom 'koordinat_fat' ada
if 'koordinat_fat' in aset_data.columns:
    print("ℹ️ Memproses kolom 'koordinat_fat' (gabungan Latitude, Longitude)...")
    try:
        # Langkah 1: Pembersihan awal pada string koordinat
        aset_data["koordinat_fat_cleaned"] = aset_data["koordinat_fat"].apply(apply_cleaning)
        print("✅ apply_cleaning pada 'koordinat_fat' selesai.")

        # Langkah 2: Pisahkan string koordinat menjadi Latitude dan Longitude
        split_coords = aset_data["koordinat_fat_cleaned"].str.split(',', expand=True)
        print("✅ Split 'koordinat_fat_cleaned' selesai.")

        # Langkah 3: Inisialisasi kolom latitude_fat dan longitude_fat dengan NaN
        aset_data["latitude_fat"] = np.nan
        aset_data["longitude_fat"] = np.nan

        # Tugaskan hasil split ke kolom target
        if split_coords.shape[1] > 0:
            aset_data["latitude_fat"] = split_coords[0]
            print("✅ Kolom 0 hasil split ditugaskan ke latitude_fat.")

        if split_coords.shape[1] > 1:
            aset_data["longitude_fat"] = split_coords[1]
            print("✅ Kolom 1 hasil split ditugaskan ke longitude_fat.")
        else:
            print("⚠️ Split tidak menghasilkan 2 kolom, longitude_fat mungkin diisi NaN atau tidak berubah.")

        # Langkah 4: Pembersihan dan konversi akhir
        print("ℹ️ Melakukan pembersihan dan konversi akhir pada latitude_fat dan longitude_fat.")
        aset_data["latitude_fat"] = aset_data["latitude_fat"].astype(str)
        aset_data["longitude_fat"] = aset_data["longitude_fat"].astype(str)
        print("✅ latitude_fat dan longitude_fat diubah ke string.")

        aset_data["latitude_fat"] = aset_data["latitude_fat"].replace(['', '<NA>', 'None'], np.nan).str.strip()
        aset_data["longitude_fat"] = aset_data["longitude_fat"].replace(['', '<NA>', 'None'], np.nan).str.strip()
        print("✅ String kosong/'<NA>'/'None'/spasi di-replace dengan NaN/dihapus.")

        aset_data["latitude_fat"] = aset_data["latitude_fat"].apply(clean_invalid_characters)
        aset_data["longitude_fat"] = aset_data["longitude_fat"].apply(clean_invalid_characters)
        print("✅ clean_invalid_characters selesai pada latitude_fat dan longitude_fat.")

        aset_data["latitude_fat"] = pd.to_numeric(aset_data["latitude_fat"], errors="coerce")
        aset_data["longitude_fat"] = pd.to_numeric(aset_data["longitude_fat"], errors="coerce")
        print("✅ Konversi akhir ke float selesai pada latitude_fat dan longitude_fat.")

        # Hitung jumlah baris non-null awal dan setelah proses
        initial_coord_non_null = aset_data["koordinat_fat"].notna().sum()
        final_lat_null_count = aset_data["latitude_fat"].isna().sum()
        final_lon_null_count = aset_data["longitude_fat"].isna().sum()
        print(f"ℹ️ Total baris: {len(aset_data)}")
        print(f"ℹ️ Baris 'koordinat_fat' non-kosong awal: {initial_coord_non_null}")
        print(f"ℹ️ Baris 'latitude_fat' yang menjadi kosong (NaN): {final_lat_null_count}")
        print(f"ℹ️ Baris 'longitude_fat' yang menjadi kosong (NaN): {final_lon_null_count}")

        # Drop kolom sementara jika tidak lagi dibutuhkan
        if "koordinat_fat_cleaned" in aset_data.columns:
            aset_data.drop(columns=["koordinat_fat_cleaned", "koordinat_fat"], inplace=True)
            print("✅ Kolom 'koordinat_fat_cleaned' dihapus.")

    except Exception as e:
        print(f"❌ Error dalam proses cleansing koordinat FAT: {e}")
        if "latitude_fat" not in aset_data.columns:
            aset_data["latitude_fat"] = np.nan
        if "longitude_fat" not in aset_data.columns:
            aset_data["longitude_fat"] = np.nan
        print("⚠️ Kolom latitude_fat dan longitude_fat mungkin tidak terproses sepenuhnya karena error.")

else:
    print("⚠️ Kolom 'koordinat_fat' tidak ditemukan dalam aset_data. Melewatkan cleansing koordinat FAT.")

# Output informasi
print("\n📋 Info aset_data setelah cleansing koordinat:")
aset_data.info()

if "latitude_fat" in aset_data.columns and "longitude_fat" in aset_data.columns:
    print("\n📋 Info kolom 'latitude_fat' dan 'longitude_fat' setelah cleansing:")
    print(aset_data[["latitude_fat", "longitude_fat"]].info())
else:
    print("\n⚠️ Kolom 'latitude_fat' atau 'longitude_fat' tidak ditemukan setelah cleansing.")

print("\n========================= Cleansing Kolom Koordinat =========================\n")

# Asumsi kolom 'koordinat_cluster' ada
if 'koordinat_cluster' in aset_data.columns:
    print("ℹ️ Memproses kolom 'koordinat_cluster' (gabungan Latitude, Longitude)...")
    try:
        # Langkah 1: Pembersihan awal pada string koordinat
        aset_data["koordinat_cluster_cleaned"] = aset_data["koordinat_cluster"].apply(apply_cleaning)
        print("✅ apply_cleaning pada 'koordinat_cluster' selesai.")

        # Langkah 2: Pisahkan string koordinat menjadi Latitude dan Longitude
        split_coords = aset_data["koordinat_cluster_cleaned"].str.split(',', expand=True)
        print("✅ Split 'koordinat_cluster_cleaned' selesai.")

        # Langkah 3: Inisialisasi kolom latitude_cluster dan longitude_cluster dengan NaN
        aset_data["latitude_cluster"] = np.nan
        aset_data["longitude_cluster"] = np.nan

        # Tugaskan hasil split ke kolom target
        if split_coords.shape[1] > 0:
            aset_data["latitude_cluster"] = split_coords[0]
            print("✅ Kolom 0 hasil split ditugaskan ke latitude_cluster.")

        if split_coords.shape[1] > 1:
            aset_data["longitude_cluster"] = split_coords[1]
            print("✅ Kolom 1 hasil split ditugaskan ke longitude_cluster.")
        else:
            print("⚠️ Split tidak menghasilkan 2 kolom, longitude_cluster mungkin diisi NaN atau tidak berubah.")

        # Langkah 4: Pembersihan dan konversi akhir
        print("ℹ️ Melakukan pembersihan dan konversi akhir pada latitude_cluster dan longitude_cluster.")
        aset_data["latitude_cluster"] = aset_data["latitude_cluster"].astype(str)
        aset_data["longitude_cluster"] = aset_data["longitude_cluster"].astype(str)
        print("✅ latitude_cluster dan longitude_cluster diubah ke string.")

        aset_data["latitude_cluster"] = aset_data["latitude_cluster"].replace(['', '<NA>', 'None'], np.nan).str.strip()
        aset_data["longitude_cluster"] = aset_data["longitude_cluster"].replace(['', '<NA>', 'None'], np.nan).str.strip()
        print("✅ String kosong/'<NA>'/'None'/spasi di-replace dengan NaN/dihapus.")

        aset_data["latitude_cluster"] = aset_data["latitude_cluster"].apply(clean_invalid_characters)
        aset_data["longitude_cluster"] = aset_data["longitude_cluster"].apply(clean_invalid_characters)
        print("✅ clean_invalid_characters selesai pada latitude_cluster dan longitude_cluster.")

        aset_data["latitude_cluster"] = pd.to_numeric(aset_data["latitude_cluster"], errors="coerce")
        aset_data["longitude_cluster"] = pd.to_numeric(aset_data["longitude_cluster"], errors="coerce")
        print("✅ Konversi akhir ke float selesai pada latitude_cluster dan longitude_cluster.")

        # Hitung jumlah baris non-null awal dan setelah proses
        initial_coord_non_null = aset_data["koordinat_cluster"].notna().sum()
        final_lat_null_count = aset_data["latitude_cluster"].isna().sum()
        final_lon_null_count = aset_data["longitude_cluster"].isna().sum()
        print(f"ℹ️ Total baris: {len(aset_data)}")
        print(f"ℹ️ Baris 'koordinat_cluster' non-kosong awal: {initial_coord_non_null}")
        print(f"ℹ️ Baris 'latitude_cluster' yang menjadi kosong (NaN): {final_lat_null_count}")
        print(f"ℹ️ Baris 'longitude_cluster' yang menjadi kosong (NaN): {final_lon_null_count}")

        # Drop kolom sementara jika tidak lagi dibutuhkan
        if "koordinat_cluster_cleaned" in aset_data.columns:
            aset_data.drop(columns=["koordinat_cluster_cleaned", "koordinat_cluster"], inplace=True)
            print("✅ Kolom 'koordinat_cluster_cleaned' dihapus.")

    except Exception as e:
        print(f"❌ Error dalam proses cleansing koordinat CLUSTER: {e}")
        if "latitude_cluster" not in aset_data.columns:
            aset_data["latitude_cluster"] = np.nan
        if "longitude_cluster" not in aset_data.columns:
            aset_data["longitude_cluster"] = np.nan
        print("⚠️ Kolom latitude_cluster dan longitude_cluster mungkin tidak terproses sepenuhnya karena error.")

else:
    print("⚠️ Kolom 'koordinat_cluster' tidak ditemukan dalam aset_data. Melewatkan cleansing koordinat CLUSTER.")

# Output informasi
print("\n📋 Info aset_data setelah cleansing koordinat:")
aset_data.info()

if "latitude_cluster" in aset_data.columns and "longitude_cluster" in aset_data.columns:
    print("\n📋 Info kolom 'latitude_cluster' dan 'longitude_cluster' setelah cleansing:")
    print(aset_data[["latitude_cluster", "longitude_cluster"]].info())
else:
    print("\n⚠️ Kolom 'latitude_cluster' atau 'longitude_cluster' tidak ditemukan setelah cleansing.")

print("\n========================= Cleansing Kolom Koordinat =========================\n")

# Asumsi kolom 'koordinat_pelanggan' ada di user_data
if 'koordinat_pelanggan' in user_data.columns:
    print("ℹ️ Memproses kolom 'koordinat_pelanggan' (gabungan Latitude, Longitude) di user_data...")
    try:
        # Langkah 1: Pembersihan awal pada string koordinat
        user_data["koordinat_pelanggan_cleaned"] = user_data["koordinat_pelanggan"].apply(apply_cleaning)
        print("✅ apply_cleaning pada 'koordinat_pelanggan' selesai.")

        # Langkah 2: Pisahkan string koordinat menjadi Latitude dan Longitude
        split_coords = user_data["koordinat_pelanggan_cleaned"].str.split(',', expand=True)
        print("✅ Split 'koordinat_pelanggan_cleaned' selesai.")

        # Langkah 3: Inisialisasi kolom latitude_pelanggan dan longitude_pelanggan dengan NaN
        user_data["latitude_pelanggan"] = np.nan
        user_data["longitude_pelanggan"] = np.nan

        # Tugaskan hasil split ke kolom target
        if split_coords.shape[1] > 0:
            user_data["latitude_pelanggan"] = split_coords[0]
            print("✅ Kolom 0 hasil split ditugaskan ke latitude_pelanggan.")

        if split_coords.shape[1] > 1:
            user_data["longitude_pelanggan"] = split_coords[1]
            print("✅ Kolom 1 hasil split ditugaskan ke longitude_pelanggan.")
        else:
            print("⚠️ Split tidak menghasilkan 2 kolom, longitude_pelanggan mungkin diisi NaN atau tidak berubah.")

        # Langkah 4: Pembersihan dan konversi akhir
        print("ℹ️ Melakukan pembersihan dan konversi akhir pada latitude_pelanggan dan longitude_pelanggan.")
        user_data["latitude_pelanggan"] = user_data["latitude_pelanggan"].astype(str)
        user_data["longitude_pelanggan"] = user_data["longitude_pelanggan"].astype(str)
        print("✅ latitude_pelanggan dan longitude_pelanggan diubah ke string.")

        user_data["latitude_pelanggan"] = user_data["latitude_pelanggan"].replace(['', '<NA>', 'None'], np.nan).str.strip()
        user_data["longitude_pelanggan"] = user_data["longitude_pelanggan"].replace(['', '<NA>', 'None'], np.nan).str.strip()
        print("✅ String kosong/'<NA>'/'None'/spasi di-replace dengan NaN/dihapus.")

        user_data["latitude_pelanggan"] = user_data["latitude_pelanggan"].apply(clean_invalid_characters)
        user_data["longitude_pelanggan"] = user_data["longitude_pelanggan"].apply(clean_invalid_characters)
        print("✅ clean_invalid_characters selesai pada latitude_pelanggan dan longitude_pelanggan.")

        user_data["latitude_pelanggan"] = pd.to_numeric(user_data["latitude_pelanggan"], errors="coerce")
        user_data["longitude_pelanggan"] = pd.to_numeric(user_data["longitude_pelanggan"], errors="coerce")
        print("✅ Konversi akhir ke float selesai pada latitude_pelanggan dan longitude_pelanggan.")

        # Hitung jumlah baris non-null awal dan setelah proses
        initial_coord_non_null = user_data["koordinat_pelanggan"].notna().sum()
        final_lat_null_count = user_data["latitude_pelanggan"].isna().sum()
        final_lon_null_count = user_data["longitude_pelanggan"].isna().sum()
        print(f"ℹ️ Total baris: {len(user_data)}")
        print(f"ℹ️ Baris 'koordinat_pelanggan' non-kosong awal: {initial_coord_non_null}")
        print(f"ℹ️ Baris 'latitude_pelanggan' yang menjadi kosong (NaN): {final_lat_null_count}")
        print(f"ℹ️ Baris 'longitude_pelanggan' yang menjadi kosong (NaN): {final_lon_null_count}")

        # Drop kolom sementara jika tidak lagi dibutuhkan
        if "koordinat_pelanggan_cleaned" in user_data.columns:
            user_data.drop(columns=["koordinat_pelanggan_cleaned", "koordinat_pelanggan"], inplace=True)
            print("✅ Kolom 'koordinat_pelanggan_cleaned' dihapus.")

    except Exception as e:
        print(f"❌ Error dalam proses cleansing koordinat PELANGGAN: {e}")
        if "latitude_pelanggan" not in user_data.columns:
            user_data["latitude_pelanggan"] = np.nan
        if "longitude_pelanggan" not in user_data.columns:
            user_data["longitude_pelanggan"] = np.nan
        print("⚠️ Kolom latitude_pelanggan dan longitude_pelanggan mungkin tidak terproses sepenuhnya karena error.")

else:
    print("⚠️ Kolom 'koordinat_pelanggan' tidak ditemukan dalam user_data. Melewatkan cleansing koordinat PELANGGAN.")

# Output informasi
print("\n📋 Info user_data setelah cleansing koordinat:")
user_data.info()

if "latitude_pelanggan" in user_data.columns and "longitude_pelanggan" in user_data.columns:
    print("\n📋 Info kolom 'latitude_pelanggan' dan 'longitude_pelanggan' setelah cleansing:")
    print(user_data[["latitude_pelanggan", "longitude_pelanggan"]].info())
else:
    print("\n⚠️ Kolom 'latitude_pelanggan' atau 'longitude_pelanggan' tidak ditemukan setelah cleansing.")

# ========================= Split Data dari aset_data =========================
print("\n=== Splitting Data from aset_data ===")

# 1. Split Data untuk Tabel user_terminals (Tabel Utama yang Direferensi)
# Blok ini HARUS dieksekusi PERTAMA sebelum memfilter pelanggan_data
user_terminals = aset_data[[
    "hostname_olt", "latitude_olt", "longitude_olt", "brand_olt", "type_olt",
    "kapasitas_olt", "kapasitas_port_olt", "olt_port", "olt", "interface_olt",
    "fdt_id", "status_osp_amarta_fdt", "jumlah_splitter_fdt", "kapasitas_splitter_fdt",
    "fdt_new_existing", "port_fdt", "latitude_fdt", "longitude_fdt",
    "fat_id", "jumlah_splitter_fat", "kapasitas_splitter_fat", "latitude_fat", "longitude_fat",
    "status_osp_amarta_fat", "fat_kondisi", "fat_filter_pemakaian", "keterangan_full",
    "fat_id_x", "filter_fat_cap"
]].copy() # Gunakan .copy() untuk menghindari SettingWithCopyWarning
print("✅ Split user_terminals from aset_data.")


# 2. Split Data untuk Tabel Clusters (Mereferensi user_terminals.fat_id)
cluster_data = aset_data[[
    "latitude_cluster", "longitude_cluster", "area_kp", "kota_kab", "kecamatan",
    "kelurahan", "up3", "ulp", "fat_id" # Kolom fat_id tetap disertakan karena Foreign Key
]].copy()
print("✅ Split cluster_data from aset_data.")

# 3. Split Data untuk Tabel Home Connected (Mereferensi user_terminals.fat_id)
home_connected_data = aset_data[[
    "hc_old", "hc_icrm", "total_hc", "cleansing_hp", "fat_id" # Kolom fat_id tetap disertakan
]].copy()
print("✅ Split home_connected_data from aset_data.")

# 4. Split Data untuk Tabel Dokumentasi (Mereferensi user_terminals.fat_id)
dokumentasi_data = aset_data[[
    "status_osp_amarta_fat", "link_dokumen_feeder", "keterangan_dokumen",
    "link_data_aset", "keterangan_data_aset", "link_maps", "update_aset",
    "amarta_update", "fat_id" # Kolom fat_id tetap disertakan
]].copy()
print("✅ Split dokumentasi_data from aset_data.")

# 5. Split Data untuk Tabel Additional Information (Mereferensi user_terminals.fat_id)
additional_info_data = aset_data[[
    "pa", "tanggal_rfs", "mitra", "kategori", "sumber_datek", "fat_id" # Kolom fat_id tetap disertakan
]].copy()
print("✅ Split additional_info_data from aset_data.")


# ========================= Split dan Filter Data dari user_data =========================
print("\n=== Splitting dan Filtering Data from user_data ===")

# Split Data untuk Tabel pelanggans (DataFrame Sementara)
# Ini adalah data pelanggan yang akan divalidasi fat_id-nya
pelanggan_data = user_data[[
    "id_permohonan", "sid", "cust_name", "telpn",
    "latitude_pelanggan", "longitude_pelanggan", "fat_id", "notes" # Kolom fat_id dari user_data
]].copy()
print("✅ Split pelanggan_data from user_data.")

# --- Modify the validation logic here ---

# Ambil daftar fat_id unik dari user_terminals (termasuk format range seperti "ID1 - ID2")
# Pastikan kolom 'fat_id' ada di user_terminals
if 'fat_id' in user_terminals.columns:
    unique_fat_ids_from_assets = user_terminals["fat_id"].unique()
    print(f"ℹ️ Ditemukan {len(unique_fat_ids_from_assets)} unique fat_ids/ranges di user_terminals (dari aset_data).")

    # Buat set baru untuk menyimpan SEMUA individual FAT ID yang valid
    expanded_valid_fat_ids = set()

    # Regex untuk mengenali format range seperti "ID1 - ID2"
    # r'^' -> mulai string
    # '\s*' -> nol atau lebih spasi
    # '([\w\d]+)' -> Tangkap grup 1: satu atau lebih karakter word (alphanumeric) atau digit
    # '\s*-\s*' -> spasi opsional, tanda hubung, spasi opsional sebagai pemisah
    # '([\w\d]+)' -> Tangkap grup 2: ID kedua
    # '\s*$' -> spasi opsional, akhir string
    range_pattern = re.compile(r'^\s*([\w\d]+)\s*-\s*([\w\d]+)\s*$')

    # Iterasi melalui setiap nilai unik fat_id/range dari aset
    for fat_id_item in unique_fat_ids_from_assets:
        # Hanya proses nilai yang berupa string dan tidak null
        if isinstance(fat_id_item, str) and pd.notna(fat_id_item):
            fat_id_str = fat_id_item.strip() # Hapus spasi di awal/akhir

            if fat_id_str == '': # Abaikan string kosong setelah strip
                continue

            # Coba cocokkan dengan pola range "ID1 - ID2"
            match = range_pattern.match(fat_id_str)
            if match:
                # Jika cocok dengan pola range, ambil kedua ID dan tambahkan ke set
                id1 = match.group(1)
                id2 = match.group(2)
                expanded_valid_fat_ids.add(id1)
                expanded_valid_fat_ids.add(id2)
                # Catatan: Jika ada format range lain (misal "ID1,ID2,ID3" atau "ID1/ID2"),
                # Anda perlu menambahkan logika atau regex lain di sini untuk menanganinya.
            else:
                # Jika tidak cocok dengan pola range, anggap itu ID tunggal dan tambahkan ke set
                expanded_valid_fat_ids.add(fat_id_str)
        # Nilai yang bukan string (misal NaN atau None) otomatis dilewati oleh isinstance(..., str)

    print(f"ℹ️ Set individual fat_id yang valid diperluas menjadi {len(expanded_valid_fat_ids)} ID unik.")

    # Filter data pelanggan untuk hanya menyimpan baris dengan fat_id yang valid
    # Sekarang gunakan expanded_valid_fat_ids untuk filtering
    # Pastikan kolom fat_id di pelanggan_data juga distandardisasi (misal: string dan strip)
    if 'fat_id' in pelanggan_data.columns:
        # Buat kolom string sementara dari fat_id pelanggan untuk perbandingan yang aman
        pelanggan_data['fat_id_str_compare'] = pelanggan_data['fat_id'].astype(str).str.strip()

        # Filter berdasarkan set expanded_valid_fat_ids
        valid_pelanggan_data = pelanggan_data[pelanggan_data["fat_id_str_compare"].isin(expanded_valid_fat_ids)].copy()
        print(f"✅ Difilter: {len(valid_pelanggan_data)} baris di pelanggan_data memiliki fat_id valid (termasuk dari range).")

        # Periksa data pelanggan dengan fat_id yang tidak valid (opsional, untuk debugging)
        invalid_pelanggan_data = pelanggan_data[~pelanggan_data["fat_id_str_compare"].isin(expanded_valid_fat_ids)].copy()
        jumlah_invalid_fat_ids = invalid_pelanggan_data.shape[0]
        print(f"⚠️ Jumlah baris di pelanggan_data dengan fat_id TIDAK valid: {jumlah_invalid_fat_ids}")

        # Bersihkan kolom string sementara
        pelanggan_data.drop(columns=['fat_id_str_compare'], inplace=True)
        valid_pelanggan_data.drop(columns=['fat_id_str_compare'], inplace=True)
        invalid_pelanggan_data.drop(columns=['fat_id_str_compare'], inplace=True)

        # Opsional: Tampilkan info baris invalid jika ada
        if not invalid_pelanggan_data.empty:
             print("❌ Contoh baris di pelanggan_data dengan fat_id tidak valid:")
             # Batasi jumlah yang ditampilkan jika banyak
             print(invalid_pelanggan_data.head())
             # Atau tampilkan nilai fat_id saja dari yang invalid
             print(invalid_pelanggan_data['fat_id'].unique())


    else: # Jika kolom 'fat_id' tidak ada di pelanggan_data (kemungkinan ada masalah di proses sebelumnya)
        print("❌ Kolom 'fat_id' tidak ditemukan di pelanggan_data. Filtering berdasarkan fat_id dilewati.")
        # Dalam kasus ini, semua data pelanggan dianggap 'invalid' berdasarkan kriteria fat_id
        valid_pelanggan_data = pd.DataFrame(columns=pelanggan_data.columns) # Buat DataFrame kosong dengan kolom yang benar
        invalid_pelanggan_data = pelanggan_data.copy() # Semua data asli pelanggan
        jumlah_invalid_fat_ids = len(invalid_pelanggan_data)
        print(f"⚠️ Semua ({jumlah_invalid_fat_ids}) baris pelanggan dianggap memiliki fat_id tidak valid.")

else: # Jika kolom 'fat_id' tidak ada di user_terminals (kemungkinan ada masalah di proses sebelumnya)
    print("❌ Kolom 'fat_id' tidak ditemukan di user_terminals. Filtering pelanggan_data berdasarkan fat_id dilewati.")
    # Dalam kasus ini, tidak ada daftar fat_id valid yang bisa dibuat, sehingga semua pelanggan dianggap invalid
    expanded_valid_fat_ids = set() # Set kosong
    valid_pelanggan_data = pd.DataFrame(columns=pelanggan_data.columns) # Buat DataFrame pelanggan valid yang kosong
    invalid_pelanggan_data = pelanggan_data.copy() # Semua data asli pelanggan
    jumlah_invalid_fat_ids = len(invalid_pelanggan_data)
    print(f"⚠️ Tidak dapat membuat daftar fat_id valid dari user_terminals. Semua ({jumlah_invalid_fat_ids}) baris pelanggan dianggap memiliki fat_id tidak valid.")


# ========================= Info DataFrame Siap Impor (Opsional) =========================

# Anda mungkin ingin mencetak info DataFrame sebelum impor untuk verifikasi
print("\n=== INFO DATAFRAME SIAP IMPOR ===")

print("\n📋 Info user_terminals DataFrame (Siap impor ke 'user_terminals'):")
user_terminals.info()

print("\n📋 Info cluster_data DataFrame (Siap impor ke 'clusters'):")
cluster_data.info()

print("\n📋 Info home_connected_data DataFrame (Siap impor ke 'home_connecteds'):")
home_connected_data.info()

print("\n📋 Info dokumentasi_data DataFrame (Siap impor ke 'dokumentasis'):")
dokumentasi_data.info()

print("\n📋 Info additional_info_data DataFrame (Siap impor ke 'additional_informations'):")
additional_info_data.info()

print("\n📋 Info valid_pelanggan_data DataFrame (Siap impor ke 'pelanggans'):")
# Ini adalah DataFrame yang sudah difilter dan akan diimpor ke tabel pelanggans
valid_pelanggan_data.info()

# Opsional: Info DataFrame pelanggan yang TIDAK valid
print("\n📋 Info invalid_pelanggan_data DataFrame (TIDAK akan diimpor ke 'pelanggans'):")
invalid_pelanggan_data.info()

# Simpan data ke file CSV
user_terminals.to_csv("data_clean/fiber.csv", index=False)
print("✅ Data OLT berhasil disimpan ke data_clean/OLT.csv")
pelanggan_data.to_csv("data_clean/Pelanggan.csv", index=False)
print("✅ Data Pelanggan berhasil disimpan ke data_clean/Pelanggan.csv")
cluster_data.to_csv("data_clean/Clusters.csv", index=False)
print("✅ Data Clusters berhasil disimpan ke data_clean/Clusters.csv")
home_connected_data.to_csv("data_clean/HomeConnected.csv", index=False)
print("✅ Data Home Connected berhasil disimpan ke data_clean/HomeConnected.csv")
dokumentasi_data.to_csv("data_clean/Dokumentasi.csv", index=False)
print("✅ Data Dokumentasi berhasil disimpan ke data_clean/Dokumentasi.csv")
additional_info_data.to_csv("data_clean/AdditionalInformation.csv", index=False)
print("✅ Data Additional Information berhasil disimpan ke data_clean/AdditionalInformation.csv")

aset_data.to_csv("aset_final.csv", index=False)

import shutil

# Buat ZIP dari folder 'data_clean'
shutil.make_archive("data_clean", 'zip', "data_clean")

import os
import glob

# Path folder yang ingin dibersihkan
folder_path = "data_clean"

# Ambil semua file di dalam folder
files = glob.glob(os.path.join(folder_path, "*"))

# Hapus satu per satu
for file in files:
    os.remove(file)

print("✅ Semua file di dalam folder 'data_clean' berhasil dihapus.")

!pip install sqlalchemy psycopg2-binary

# Library untuk koneksi database
import sqlalchemy
import psycopg2

# Diasumsikan DataFrame (user_terminals, cluster_data, etc.) sudah dibuat/diproses sebelumnya.
# Diasumsikan fungsi pembersihan data sudah didefinisikan di awal script.

# ========================= Database Connection Setup =========================
print("\n=== DATABASE CONNECTION SETUP ===")

# Mendapatkan kredensial database dari Environment Variables atau Colab Secrets
# Sesuaikan bagian ini dengan cara Anda menyimpan secrets
try:
    # Jika Anda menggunakan google.colab.userdata secara langsung:
    DB_HOST = userdata.get("SUPABASE_DB_HOST")
    DB_DATABASE = userdata.get("SUPABASE_DB_NAME")
    DB_USER = userdata.get("SUPABASE_DB_USER")
    DB_PASSWORD = userdata.get("SUPABASE_DB_PASSWORD")
    DB_PORT = userdata.get("SUPABASE_DB_PORT")

    if not all([DB_HOST, DB_DATABASE, DB_USER, DB_PASSWORD]):
        raise ValueError("Database credentials incomplete. Check SUPABASE_DB_HOST, SUPABASE_DB_NAME, SUPABASE_DB_USER, SUPABASE_DB_PASSWORD.")

    print("✅ Database credentials loaded successfully.")

except (KeyError, ValueError) as e:
    print(f"❌ Failed to load database credentials: {e}")
    print("ℹ️ Please ensure Colab Secrets or Environment Variables are set correctly.")
    # Anda mungkin ingin menghentikan eksekusi jika kredensial tidak ada
    # exit()
    engine = None # Set engine menjadi None agar impor dilewati
except Exception as e:
    print(f"❌ An unexpected error occurred while loading credentials: {e}")
    engine = None # Set engine menjadi None agar impor dilewati

# Membuat Database URL untuk SQLAlchemy
# Gunakan 'postgresql' sebagai dialek
if engine is None: # Hanya lanjutkan jika kredensial berhasil dimuat
    db_url = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_DATABASE}"
    print(f"ℹ️ Attempting to connect to: postgresql://{DB_USER}:******@{DB_HOST}:{DB_PORT}/{DB_DATABASE}")

    # Membuat SQLAlchemy engine
    try:
        # pool_recycle handles connection timeouts (common in cloud databases)
        engine = sqlalchemy.create_engine(db_url, pool_recycle=3600)
        print("✅ SQLAlchemy engine created successfully.")
        # Opsional: Cek koneksi
        # with engine.connect() as connection:
        #     print("✅ Database connection test successful.")
    except Exception as e:
        print(f"❌ Failed to create SQLAlchemy engine or connect: {e}")
        print("ℹ️ Please check your credentials, host, port, database name, and ensure the database is accessible.")
        # Anda mungkin ingin menghentikan eksekusi jika koneksi gagal
        engine = None # Pastikan engine None jika gagal


# ========================= Data Import Order =========================
# Definisikan urutan impor berdasarkan relasi Foreign Key.
# Tabel yang menjadi referensi (user_terminals) HARUS diimpor duluan.
# Tabel lain yang mereferensi user_terminals dapat diimpor setelahnya.
import_order = [
    {"df": user_terminals, "table_name": "user_terminals"},
    {"df": cluster_data, "table_name": "clusters"},
    {"df": home_connected_data, "table_name": "home_connecteds"},
    {"df": dokumentasi_data, "table_name": "dokumentasis"},
    {"df": additional_info_data, "table_name": "additional_informations"},
    # Gunakan DataFrame pelanggan yang sudah difilter (valid_pelanggan_data)
    # karena tabel pelanggans mereferensi user_terminals(fat_id) dengan NOT NULL
    {"df": valid_pelanggan_data, "table_name": "pelanggans"},
]

# ========================= Import DataFrames to Supabase =========================
print("\n=== IMPORTING DATA TO SUPABASE ===")

# Definisikan perilaku jika tabel sudah ada:
# 'fail': Akan error jika tabel sudah ada.
# 'replace': Tabel akan dihapus lalu dibuat ulang sebelum data diimpor.
# 'append': Data akan ditambahkan ke tabel yang sudah ada.
# Pilih 'append' jika Anda ingin menambahkan data baru tanpa menghapus data lama.
# Pilih 'replace' jika Anda ingin me-reload total data di tabel (hati-hati!).
if_exists_behavior = 'append' # <<-- SESUAIKAN INI ('append' atau 'replace')

if engine is not None: # Lanjutkan hanya jika engine berhasil dibuat
    for item in import_order:
        df_to_import = item["df"]
        table_name = item["table_name"]

        print(f"\nℹ️ Importing data to table '{table_name}'...")

        # Periksa apakah DataFrame kosong sebelum diimpor
        if df_to_import is None or not isinstance(df_to_import, pd.DataFrame) or df_to_import.empty:
            print(f"⚠️ DataFrame for '{table_name}' is empty or invalid. Skipping import.")
            continue

        try:
            # Menggunakan metode to_sql pandas
            # name: Nama tabel di database.
            # con: SQLAlchemy engine atau koneksi database.
            # if_exists: Perilaku jika tabel sudah ada ('append' atau 'replace').
            # index=False: Tidak memasukkan index DataFrame sebagai kolom di tabel.
            # dtype: Opsional, mapping eksplisit tipe data Pandas ke tipe data SQL.
            #        Pandas/SQLAlchemy biasanya cukup pintar, tapi bisa dispesifikasikan
            #        untuk kontrol lebih (misal: pd.Int64 -> sqlalchemy.BigInteger).
            #        Untuk SERIAL PRIMARY KEY, kolom tersebut TIDAK BOLEH ADA di DataFrame
            #        karena database yang generate nilainya. DataFrame Anda sudah benar.
            df_to_import.to_sql(
                name=table_name,
                con=engine,
                if_exists=if_exists_behavior,
                index=False
                # Contoh penggunaan dtype eksplisit:
                # , dtype={
                #     "tanggal_rfs": sqlalchemy.types.Date,
                #     "kapasitas_olt": sqlalchemy.BigInteger # Menggunakan BigInteger untuk pd.Int64
                # }
            )
            print(f"✅ Successfully imported {len(df_to_import)} rows to '{table_name}'.")

        except Exception as e:
            print(f"❌ Failed to import data to '{table_name}': {e}")
            print("ℹ️ Check for potential issues:")
            print("   - Table schema matching DataFrame columns (names, order, case-sensitivity).")
            print("   - Data types compatibility (e.g., unexpected values in INTEGER columns, invalid dates).")
            print("   - Foreign key violations (ensure fat_id in dependent tables exists in user_terminals.fat_id, especially for NOT NULL constraints).")
            print("   - Primary key violations (ensure no duplicate fat_id in user_terminals or id_permohonan in pelanggans).")
            print("   - Database connection stability.")
            # Jika terjadi error impor, Anda mungkin ingin menghentikan impor tabel berikutnya
            # jika relasinya penting. Atau biarkan lanjut untuk melihat error di tabel lain.
            # break # Hentikan loop impor jika terjadi error

else:
    print("\n⚠️ Data import skipped because database connection could not be established.")


# ========================= Close Connection (Good Practice) =========================
# SQLAlchemy engine mengelola koneksi dalam pool. dispose() akan menutup semua koneksi di pool.
if 'engine' in locals() and engine:
    try:
        engine.dispose()
        print("\nℹ️ Database engine disposed.")
    except Exception as e:
        print(f"❌ Error disposing database engine: {e}")


# ========================= Lanjutkan dengan bagian lain dari script Anda =========================
# ... misalnya menyimpan log, analisis lebih lanjut, dll.